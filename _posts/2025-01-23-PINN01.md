---
title: PINN(01) 简介和基本原理
toc: true
categories:
  - PINN
tags:
  - 计算机模拟
  - 数值计算
---

PINN 和可微物理学习笔记（01）

<!-- more -->

此系列笔记为本人学习了解 PINN 整理所做，每个笔记都比较短小以方便更新。

## PINN 简介

Physics-Informed Neural Network (PINN) 这一工作是用神经网络来近似求解微分方程，其主要思想是用神经网络来作为一个万能的函数近似器，通过构造和微分方程有关的 Loss，将求解微分方程的问题转化成一个神经网络的优化问题，从而使用梯度下降法等常见的方法得到微分方程的近似解。因此， PINN 可以作为一种新的微分方程求解方法。

另外，如果我们同时有试验数据，我们可以在使用神经网络拟合试验数据的同时将微分方程相关的 Loss 作为网络的正则化项，从而在训练的过程中将“物理信息”添加进神经网络的训练之中，将模型的输出限制在一个比较小的取值空间中，从而使用少量的数据就能得到一个预测较为准确的网络模型，提高模型的泛化能力，同时加速模型的训练过程。

## PINN 求解框架

使用 PINN 求解微分方程可以概括为如下步骤：

对于方程：

$$
\mathcal{D}[u(\mathbf{x})] = f(\mathbf{x}), \quad \mathbf{x} \in \mathbf{\Omega} \subset \mathbb{R}^n
$$

$$
\mathcal{B}_k[u(\mathbf{x})] = g_k(\mathbf{x}), \quad \mathbf{x} \in \mathbf{\Gamma}_k \subset \partial \mathbf{\Omega}
$$

其中 $\mathcal{D}$ 为某个微分算符，其对应了微分方程的方程本身，$\mathcal{B_k}$ 则为该微分方程需要满足的边界条件。

PINN 训练一个神经网络来近似方程的解 $NN(\mathbf{x};\mathbf{\theta}) \approx u(\mathbf{x})$，通过梯度下降法或其他优化方法来调整神经网络的参数 $\mathbf{\theta}$。

PINN 的核心在于损失函数的定义，在 PINN 的训练的过程中，损失函数分为下面三部分：

- **Boundary Loss**：边界条件带来的损失

- **Physics Loss**：微分方程带来的损失

- **Data Loss**：拟合已知部分试验数据带来的损失

即损失函数为（每一个损失函数前一般都会有一个系数作为超参，这里为了简略略去）：
$$
\mathcal{L}(\mathbf{\theta}) = \mathcal{L}_b(\mathbf{\theta}) + \mathcal{L}_p(\mathbf{\theta}) + \mathcal{L}_d(\mathbf{\theta})
$$

这里 Borundary Loss 为：

$$
\mathcal{L}_b(\mathbf{\theta}) = \sum_k \frac{\lambda_{b_k}}{N_{b_k}} \sum_{j=1}^{N_{b_k}} \| \mathcal{B}_k[N(\mathbf{x}_{k,j};\mathbf{\theta})] - g_k (\mathbf{x}_{k,j}) \|
$$

其中，$\lambda_{b_k}$ 为每一个边界条件的系数，其为训练时的超参，$N_{b_k}$ 为落在第 $k$ 个边界条件上的点的总数，$\mathbf{x}_{k,j}$ 为在第 $k$ 个边界条件的第 $j$ 个点. 简而言之，就是网络在边界处各点的根据边界条件计算出的值和实际的值的均方误差（MSE）

Data Loss 定义为：

$$
\mathcal{L}_d(\mathbf{\theta}) = \frac{1}{N_d} \sum_{k=1}^{N_d} \| N(\mathbf{x}_k;\mathbf{\theta}) - \mathbf{u}_k \|
$$

其中，$N_d$ 为数据的总数，$\mathbf{u}_k$ 则为在对应点 $\mathbf{x}_k$ 的已知数据（其一般来源于实验测量计算或传统的数值计算方法）。

这里实际上并不要求 $\mathbf{u}_k$ 是完全精确的微分方程的解 $u(\mathbf{x})$ 的函数值，其可以有一定的误差和噪声，PINN 训练的过程其也相当于对原始数据进行”降噪“。

实际上，Boundary Loss 可以看成是 Data Loss 的一部分（尤其是直接给定解函数在边界上的函数值的 Dirichlet 边界条件），在原始论文中也没有对其进行区分，但这里由于其来源不同，本人还是将 Boundary Loss 和 Data Loss 进行了区分。

Physics Loss 定义为：

$$
\mathcal{L}_p (\mathbf{\theta}) = \frac{1}{N_p} \sum_{k=1}^{N_p} \| \mathcal{D}[N(\mathbf{x}_k;\mathbf{\theta})] - f(\mathbf{x}_k) \|
$$

可以看见，无论是哪一种 Loss，其都是均方误差（MSE）的形式。一个研究方向是尝试去研究是否有更好的 Loss 构造方式。另外，在 Borundary Loss（这里只针对 Neumann 边界条件和 Robin 边界条件）和 Physics Loss 中，其偏导数的计算常常直接通过现代神经网络库的自动微分实现，从而避免了人工繁琐的推导。

在训练的过程中，我们往往是在待定的求解域内使用蒙特卡诺方法随机取点采样，因此，使用 PINN 求解微分方程有一个优点是无需离散网格，无需进行网格划分，这使得其相对与传统的基于网格的数值方法相比，更适用于处理复杂的几何形状和不规则域，同时也更自由地适应问题的几何特征，从而提高模型的灵活性和准确性。
